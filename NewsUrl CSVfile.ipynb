{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import csv\n",
    "\n",
    "#각 기사 url을 담을 리스트\n",
    "url = []\n",
    "pageurlbox = []\n",
    "Max_url = 150 \n",
    "for n1 in range(10):  #10페이지니까\n",
    "    p = 1 + n1*15 #p값이 1에서 시작해 페이지당 15씩 증가하니까\n",
    "    pageurl = f'https://www.donga.com/news/search?p={p}&query=%EB%93%9C%EB%A1%A0&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'\n",
    "    pageurlbox.append(pageurl)\n",
    "\n",
    "#각 기사의 url을 담기(100개)\n",
    "while len(url) < Max_url:\n",
    "    for i in pageurlbox:\n",
    "        html = urllib.request.urlopen(i).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        findurl = soup.find_all(\"p\", class_ =\"txt\")\n",
    "        for q in findurl:\n",
    "            box = [q.find(\"a\")[\"href\"]] #writerows를 쓰기 위해서 list형태로\n",
    "            url.append(box)\n",
    "            \n",
    "#기사 url을 담은 리스트를 저장\n",
    "f = open('DongaNewsUrls.csv', 'w', newline=\"\") #newline =\"\"을 주지않으면, writerow(s)이후 줄바꿈 1줄이 자동으로 실행됨\n",
    "writer = csv.writer(f)\n",
    "writer.writerows(url)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
