{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 드론과 관련된 인식 분석 연구"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 목표 및 계획"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹 사이트에서 직접 크롤링을 통해 데이터를 획득한 뒤 가공하며 wordcloud 방식으로 시각화하여 드론과 관련된 키워드들을 뽑아 드론에 대한 인식을 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주제 선정 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고등학교 2학년 자율동아리에서 드론에 관한 인식에 대한 발표를 했는데 그것의 연장선으로 구체적으로 조사해보기 위해 주제를 선정하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가설 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹 사이트에서 드론을 검색했을 때 함께 나오는 단어들을 분석하고 시각화하면 관련된 키워드들을 꼽을 수 있을 것이고 그것이 드론의 인식과 관련있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 획득 및 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동아일보 뉴스 url csv파일로 제작 <br>\n",
    "from bs4 import BeautifulSoup <br>\n",
    "import urllib.request <br>\n",
    "import csv<br><br>\n",
    "\n",
    "#각 기사 url을 담을 리스트<br>\n",
    "url = []<br>\n",
    "pageurlbox = []<br>\n",
    "Max_url = 150 <br>\n",
    "for n1 in range(10):  #10페이지니까<br>\n",
    "    p = 1 + n1*15 #p값이 1에서 시작해 페이지당 15씩 증가하니까<br>\n",
    "    pageurl = f'https://www.donga.com/news/search?p={p}&query=%EB%93%9C%EB%A1%A0&check_news=1&more=1&sorting=1&search_date=1&v1=&v2=&range=1'<br>\n",
    "    pageurlbox.append(pageurl)<br><br>\n",
    "\n",
    "#각 기사의 url을 담기(100개)<br>\n",
    "while len(url) < Max_url:<br>\n",
    "    for i in pageurlbox:<br>\n",
    "        html = urllib.request.urlopen(i).read()<br>\n",
    "        soup = BeautifulSoup(html, 'html.parser')<br>\n",
    "        findurl = soup.find_all(\"p\", class_ =\"txt\")<br>\n",
    "        for q in findurl:<br>\n",
    "            box = [q.find(\"a\")[\"href\"]] #writerows를 쓰기 위해서 list형태로<br>\n",
    "            url.append(box)<br><br>\n",
    "            \n",
    "#기사 url을 담은 리스트를 저장<br>\n",
    "f = open('DongaNewsUrls.csv', 'w', newline=\"\") #newline =\"\"을 주지않으면, writerow(s)이후 줄바꿈 1줄이 자동으로 실행됨<br>\n",
    "writer = csv.writer(f)<br>\n",
    "writer.writerows(url)<br>\n",
    "f.close()<br><br>\n",
    "\n",
    "- 코드 원본: NewsUrl CSVfile.ipynb 결과:DongaNewsUrls.csv <br><br>\n",
    "\n",
    "#### 뉴스 url을 받아와서 각 뉴스 제목과 본문 크롤링한 후 텍스트 파일로 제작 <br>\n",
    "import csv<br>\n",
    "import urllib.request<br>\n",
    "from bs4 import BeautifulSoup<br>\n",
    "import re<br><br>\n",
    "\n",
    "#csv파일로부터 뉴스 url 읽어와서 리스트꼴로([[0],[1]..])<br>\n",
    "f = open(\"DongaNewsUrls.csv\",'r')<br>\n",
    "reader = csv.reader(f)<br>\n",
    "urlbox = [row for row in reader]<br>\n",
    "f.close()<br><br>\n",
    "\n",
    "#제목, 본문 긁어오기<br>\n",
    "titlebox = []<br>\n",
    "textbox = []<br>\n",
    "for i in range(len(urlbox)):<br>\n",
    "    url = urlbox[i][0]<br>\n",
    "    html = urllib.request.urlopen(url).read()<br>\n",
    "    soup = BeautifulSoup(html, 'html.parser')<br>\n",
    "      #제목 긁어오기(제목의 내용이 포인트니까 더 강조한다고 생각하여 제목도 포함)<br>\n",
    "    title = soup.find_all(\"h1\", class_=\"title\")<br>\n",
    "    for q in range(len(title)):<br>\n",
    "        titleNoTag = re.sub('<.+?>','',str(title),0)#<h1> 태그 제거<br>\n",
    "        titleNoList = titleNoTag.replace('[','').replace(']','') #[]제거<br>\n",
    "        titlebox.append(titleNoList)<br>\n",
    "    #본문 긁어오기<br>\n",
    "    text = soup.find_all(\"div\", class_= \"article_txt\")<br>\n",
    "    for q in range(len(text)):<br>\n",
    "        textNoTag = re.sub('<.+?>','',str(text),0)<br>\n",
    "        textNoList = textNoTag.replace('[','').replace(']','')<br>\n",
    "        textCleaning1 = re.sub('[a-zA-Z]','',textNoList) #영어 제거<br>\n",
    "        textCleaning2 = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》}{_\\n\\r]','',textCleaning1) #특수문자제거<br>\n",
    "        textbox.append(textCleaning2)<br><br>\n",
    "        \n",
    "#제목과 본문 합치기<br>\n",
    "TitleAndText = titlebox + textbox<br><br>\n",
    "\n",
    "#text 파일로 만들기<br>\n",
    "with open(\"TitleAndText.txt\",\"w\", encoding = \"utf-8\") as f:<br>\n",
    "    f.writelines(TitleAndText)<br><br>\n",
    "\n",
    "- 코드 원본: CSV file crawling.ipynb 결과:TitleAndText.txt <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트파일로 제목과 본문을 받아서 빈출단어 상위 50개 뽑아세서 <br>\n",
    "import sys<br>\n",
    "from konlpy.tag import Okt<br>\n",
    "from collections import Counter<br><br>\n",
    "\n",
    "with open('TitleAndText.txt','r', encoding= 'utf-8') as f:<br>\n",
    "    str1 = f.readlines()<br><br>\n",
    "\n",
    "#명사 분리 후 빈도 계산 함수<br>\n",
    "def get_tag(text, ntags =50): #상위 50개의 명사만 결과값으로 반환<br>\n",
    "    spliter = Okt() # Twitter  객체 생성(Twitter가 Okt로 바뀌었다고 함)<br>\n",
    "    nouns = spliter.nouns(text[0]) <br>\n",
    "    count = Counter(nouns)<br>\n",
    "    return_list = []<br>\n",
    "    for n , c in count.most_common(ntags):<br>\n",
    "        temp = {'tag': n , 'count': c}<br>\n",
    "        return_list.append(temp)<br>\n",
    "    return return_list<br><br>\n",
    "\n",
    "\n",
    "tags = get_tag(str1, 50) #리스트를 그냥 넣으면 에러가 나고 text[0]으로 하니까 됨<br>\n",
    "open_output_txt = open('VerbAnalysis.txt', 'w')<br>\n",
    "for tag in tags:<br>\n",
    "    noun = tag['tag']<br>\n",
    "    count = tag['count']<br>\n",
    "    open_output_txt.write('{} {}\\n'.format(noun, count))<br>\n",
    "open_output_txt.close()<br><br>\n",
    "\n",
    "- 코드 원본: Analysis text files with wordcloud.ipynb 결과: VerbAnalysis.txt <br><br>\n",
    "\n",
    "konlpy가 '등', '이' 같은 명사들까지 구분하지 못하였기 때문에 이런 것들은 직접 수정을 해주었음.<br>\n",
    "- 결과:VerbAnalysis(modified version).txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VerbAnalysis(modified versoin).txt을 가져와서 wordcloud로 시각화(드론 이미지로 mask)<br>\n",
    "from wordcloud import WordCloud<br>\n",
    "import matplotlib.pyplot as plt<br>\n",
    "from PIL import Image<br>\n",
    "import random<br>\n",
    "import numpy as np<br><br>\n",
    "\n",
    "#텍스트 파일 불러서 dictionary 형태로<br>\n",
    "with open('VerbAnalysis(modified version).txt','r',) as f:<br>\n",
    "    vocdict = {}<br>\n",
    "    for voc in f:<br>\n",
    "        keyvalue = voc.strip().split(\" \")<br>\n",
    "        vocdict[keyvalue[0]] = int(keyvalue[1]) #숫자가 str꼴이면 generate_from_frequencies가 작동안함<br><br>\n",
    "        \n",
    "#wordcloud 제작<br>\n",
    "\n",
    "icon = Image.open(\"pngwing.com.png\")<br>\n",
    "mask = Image.new(\"RGB\", icon.size, (255,255,255))<br>\n",
    "mask.paste(icon, icon)<br>\n",
    "mask = np.array(mask)<br>\n",
    "wordcloud = WordCloud(font_path = 'C:/Users/msi/Desktop/wordcloud aobut drone project/NanumGothic.ttf' , background_color='white', colormap =\"Accent_r\", mask = mask, max_font_size=300, random_state=42)<br>\n",
    "wc_img = wordcloud.generate_from_frequencies(vocdict)<br>\n",
    "wc_img.to_file('DroneWordcloud.jpg')<br><br>\n",
    "\n",
    "- 코드 원본: DroneWordCloud jpg.ipynb 결과:DroneWordcloud.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한계점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 구글 뉴스로 하려다가 http404에러가 떠서 헤더를 추가해서 사용하려고 했지만 잘 안되어 네이버 뉴스로 방향을 틈 -> selenium을 사용하여 해결할 수 있을 듯\n",
    "2. 뉴스의 각 페이지마다 url을 받아서 했는데 페이지마다의 공통점을 찾는다면 baseurl과 plusurl로 구성하여 더욱 깔끔하게 코딩할 수 있었을 것이다. -> 네이버의 각 페이지마다 뉴스의 url을 받아와서 진행했었는데 각 뉴스마다 html 구조가 달라서 일일히 분석을 해야하는 상황이라 동아일보로 고정해서 다시 url을 얻어와서 했음.\n",
    "3. 뉴스 기사 text에서 필요없는 부분이 본문에 포함되어 있어서 같이 검색결과가 들어갔음.\n",
    "4. KoNlpy 설치에서 에러가 많이 났는데 여러 가지 해결책을 다 적용하다가 어느 순간 잘 작동해서 오류 해결에 있어 어떤 것들이 필요했는 지 익히지 못하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 문헌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://surgach.tistory.com/4\n",
    "- https://coalcoder.tistory.com/6\n",
    "- https://yoonpunk.tistory.com/7\n",
    "- https://liveyourit.tistory.com/40\n",
    "- https://truman.tistory.com/112"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
